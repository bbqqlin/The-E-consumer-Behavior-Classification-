install.packages("plyr", dependencies = T)
library(plyr)
library(readr)

#import survey dataset (Government Office Regions)
X5661SurveyofPersonalIncomes <- read_csv("~/Desktop/Data/5661SurveyofPersonalIncomes.csv")
X6988OpinionsSurveyInternetAccessModule <- read_csv("~/Desktop/Data/6988ONSOpinionsSurveyInternetAccessModule.csv")

#import census dataset (LOSA)
IndexofMultipleDeprivation2015eng <- read_csv("~/Desktop/Data/IndexofMultipleDeprivation2015eng.csv")


#Split/subset a data frame by factors in Government Office Regions column 
5661list <- split(X5661SurveyofPersonalIncomes, X5661SurveyofPersonalIncomes$GORA)
6988list <- split(X6988OpinionsSurveyInternetAccessModule, X6988OpinionsSurveyInternetAccessModule$GORCODE)

#Merge the dataset by region
total <- merge(5661list, 6988list,by="ID")

# Standardise variables #
a<-scale(a)

# Elbow plot for best number of clusters - below #
set.seed(123)
k.max <- 15
data <- a
wss <- sapply(1:k.max,
function(k){kmeans(data, k, nstart=50 )$tot.withinss})
plot(1:k.max, wss, type="b", pch = 19, frame = FALSE, xlab="Number of clusters K", ylab="Total within-clusters sum of squares")
abline(v = 3, lty =2)

# Fit k means (Number is K) 
#Starting point. Okay, now that we have seen the data, let us try to cluster it. Since the initial cluster assignments are random, let us set the seed to ensure reproducibility.

set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)

Since we know that there are 3 species involved, we ask the algorithm to group the data into 3 clusters, and since the starting assignments are random, we specify nstart = 20. This means that R will try 20 different random starting assignments and then select the one with the lowest within cluster variation. We can see the cluster centroids, the clusters that each data point was assigned to, and the within cluster variation.
Â· Set seed gives the same random start (not sure how to pick that?)
